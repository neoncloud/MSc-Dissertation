@inproceedings{10.1007/BFb0067700,
 address = {Berlin, Heidelberg},
 author = {Mor{\'e}, Jorge J.},
 booktitle = {Numerical Analysis},
 editor = {Watson, G. A.},
 isbn = {978-3-540-35972-2},
 pages = {105--116},
 publisher = {Springer Berlin Heidelberg},
 title = {The Levenberg-Marquardt algorithm: Implementation and theory},
 year = {1978}
}

@article{10.1145/1073204.1073308,
 abstract = {This paper introduces a shading model for light diffusion in multi-layered translucent materials. Previous work on diffusion in translucent materials has assumed smooth semi-infinite homogeneous materials and solved for the scattering of light using a dipole diffusion approximation. This approximation breaks down in the case of thin translucent slabs and multi-layered materials. We present a new efficient technique based on multiple dipoles to account for diffusion in thin slabs. We enhance this multipole theory to account for mismatching indices of refraction at the top and bottom of of translucent slabs, and to model the effects of rough surfaces. To model multiple layers, we extend this single slab theory by convolving the diffusion profiles of the individual slabs. We account for multiple scattering between slabs by using a variant of Kubelka-Munk theory in frequency space. Our results demonstrate diffusion of light in thin slabs and multi-layered materials such as paint, paper, and human skin.},
 address = {New York, NY, USA},
 author = {Donner, Craig and Jensen, Henrik Wann},
 doi = {10.1145/1073204.1073308},
 issn = {0730-0301},
 issue_date = {July 2005},
 journal = {ACM Trans. Graph.},
 keywords = {realistic image synthesis, diffusion theory, BSSRDF, light transport, subsurface scattering, layered materials, reflection models, global illumination},
 number = {3},
 numpages = {8},
 pages = {1032–1039},
 publisher = {Association for Computing Machinery},
 title = {Light Diffusion in Multi-Layered Translucent Materials},
 url = {https://doi.org/10.1145/1073204.1073308},
 volume = {24},
 year = {2005}
}

@inproceedings{10.1145/1198555.1198593,
 abstract = {The ultimate challenge in photorealistic computer graphics is rendering believable human faces. We are trained to study the human face since birth, so our brains are intimately familiar with every nuance and detail of what human skin is supposed look like. The challenge of rendering human skin is further complicated by some technical issues such as the fact that skin is a highly detailed surface with noticeable features in the order of ~100 microns and the fact that skin is translucent. On The Matrix Reloaded we had to create completely photorealistic renderings for most of the principal actors including Keanu Reeves, Lawrence Fishborne, and Hugo Weaving.},
 address = {New York, NY, USA},
 author = {Borshukov, George and Lewis, J. P.},
 booktitle = {ACM SIGGRAPH 2005 Courses},
 doi = {10.1145/1198555.1198593},
 isbn = {9781450378338},
 location = {Los Angeles, California},
 pages = {13–es},
 publisher = {Association for Computing Machinery},
 series = {SIGGRAPH '05},
 title = {Realistic Human Face Rendering for "The Matrix Reloaded"},
 url = {https://doi.org/10.1145/1198555.1198593},
 year = {2005}
}

@article{10.1145/1409060.1409093,
 abstract = {We introduce a layered, heterogeneous spectral reflectance model for human skin. The model captures the inter-scattering of light among layers, each of which may have an independent set of spatially-varying absorption and scattering parameters. For greater physical accuracy and control, we introduce an infinitesimally thin absorbing layer between scattering layers. To obtain parameters for our model, we use a novel acquisition method that begins with multi-spectral photographs. By using an inverse rendering technique, along with known chromophore spectra, we optimize for the best set of parameters for each pixel of a patch. Our method finds close matches to a wide variety of inputs with low residual error.We apply our model to faithfully reproduce the complex variations in skin pigmentation. This is in contrast to most previous work, which assumes that skin is homogeneous or composed of homogeneous layers. We demonstrate the accuracy and flexibility of our model by creating complex skin visual effects such as veins, tattoos, rashes, and freckles, which would be difficult to author using only albedo textures at the skin's outer surface. Also, by varying the parameters to our model, we simulate effects from external forces, such as visible changes in blood flow within the skin due to external pressure.},
 address = {New York, NY, USA},
 articleno = {140},
 author = {Donner, Craig and Weyrich, Tim and d'Eon, Eugene and Ramamoorthi, Ravi and Rusinkiewicz, Szymon},
 doi = {10.1145/1409060.1409093},
 issn = {0730-0301},
 issue_date = {December 2008},
 journal = {ACM Trans. Graph.},
 keywords = {reflection models, subsurface scattering, light transport, skin reflectance, layered materials, BSSRDF},
 number = {5},
 numpages = {12},
 publisher = {Association for Computing Machinery},
 title = {A Layered, Heterogeneous Reflectance Model for Acquiring and Rendering Human Skin},
 url = {https://doi.org/10.1145/1409060.1409093},
 volume = {27},
 year = {2008}
}

@inbook{10.1145/3596711.3596747,
 abstract = {This paper introduces a simple model for subsurface light transport in translucent materials. The model enables efficient simulation of effects that BRDF models cannot capture, such as color bleeding within materials and diffusion of light across shadow boundaries. The technique is efficient even for anisotropic, highly scattering media that are expensive to simulate using existing methods. The model combines an exact solution for single scattering with a dipole point source diffusion approximation for multiple scattering. We also have designed a new, rapid image-based measurement technique for determining the optical properties of translucent materials. We validate the model by comparing predicted and measured values and show how the technique can be used to recover the optical properties of a variety of materials, including milk, marble, and skin. Finally, we describe sampling techniques that allow the model to be used within a conventional ray tracer.},
 address = {New York, NY, USA},
 articleno = {35},
 author = {Wann Jensen, Henrik and Marschner, Stephen R. and Levoy, Marc and Hanrahan, Pat},
 booktitle = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
 edition = {1},
 isbn = {9798400708978},
 numpages = {8},
 publisher = {Association for Computing Machinery},
 title = {A Practical Model for Subsurface Light Transport},
 url = {https://doi.org/10.1145/3596711.3596747},
 year = {2023}
}

@inproceedings{10.1145/383259.383319,
 abstract = {This paper introduces a simple model for subsurface light transport in translucent materials. The model enables efficient simulation of effects that BRDF models cannot capture, such as color bleeding within materials and diffusion of light across shadow boundaries. The technique is efficient even for anisotropic, highly scattering media that are expensive to simulate using existing methods. The model combines an exact solution for single scattering with a dipole point source diffusion approximation for multiple scattering. We also have designed a new, rapid image-based measurement technique for determining the optical properties of translucent materials. We validate the model by comparing predicted and measured values and show how the technique can be used to recover the optical properties of a variety of materials, including milk, marble, and skin. Finally, we describe sampling techniques that allow the model to be used within a conventional ray tracer.},
 address = {New York, NY, USA},
 author = {Jensen, Henrik Wann and Marschner, Stephen R. and Levoy, Marc and Hanrahan, Pat},
 booktitle = {Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques},
 doi = {10.1145/383259.383319},
 isbn = {158113374X},
 keywords = {subsurface scattering, reflection models, diffusion theory, BSSRDF, realistic image synthesis, light transport},
 numpages = {8},
 pages = {511–518},
 publisher = {Association for Computing Machinery},
 series = {SIGGRAPH '01},
 title = {A Practical Model for Subsurface Light Transport},
 url = {https://doi.org/10.1145/383259.383319},
 year = {2001}
}

@inproceedings{10.5555/2383894.2383946,
 abstract = {We present a novel spectral shading model for human skin. Our model accounts for both subsurface and surface scattering, and uses only four parameters to simulate the interaction of light with human skin. The four parameters control the amount of oil, melanin and hemoglobin in the skin, which makes it possible to match specific skin types. Using these parameters we generate custom wavelength dependent diffusion profiles for a two-layer skin model that account for subsurface scattering within the skin. These diffusion profiles are computed using convolved diffusion multipoles, enabling an accurate and rapid simulation of the subsurface scattering of light within skin. We combine the subsurface scattering simulation with a Torrance-Sparrow BRDF model to simulate the interaction of light with an oily layer at the surface of the skin. Our results demonstrate that this four parameter model makes it possible to simulate the range of natural appearance of human skin including African, Asian, and Caucasian skin types.},
 address = {Goslar, DEU},
 author = {Donner, Craig and Jensen, Henrik Wann},
 booktitle = {Proceedings of the 17th Eurographics Conference on Rendering Techniques},
 isbn = {3905673355},
 location = {Nicosia, Cyprus},
 numpages = {9},
 pages = {409–417},
 publisher = {Eurographics Association},
 series = {EGSR '06},
 title = {A {S}pectral {BSSRDF} for {S}hading {H}uman {S}kin},
 year = {2006}
}

@inproceedings{2015ExtendingTD,
 author = {Brent Burley},
 booktitle = {SIGGRAPH 2015 Course: Physically Based Shading in Theory and Practice},
 title = {Extending the Disney BRDF to a BSDF with Integrated Subsurface Scattering},
 url = {https://api.semanticscholar.org/CorpusID:208625014},
 year = {2015}
}

@inproceedings{2021arXiv210609685H,
 author = {Edward J. Hu and
Yelong Shen and
Phillip Wallis and
Zeyuan Allen{-}Zhu and
Yuanzhi Li and
Shean Wang and
Lu Wang and
Weizhu Chen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {LoRA: Low-Rank Adaptation of Large Language Models},
 url = {https://openreview.net/forum?id=nZeVKeeFYf9},
 year = {2022}
}

@article{2023arXiv230205543Z,
 author = {{Zhang}, Lvmin and {Agrawala}, Maneesh},
 journal = {ArXiv preprint},
 title = {{Adding Conditional Control to Text-to-Image Diffusion Models}},
 url = {https://arxiv.org/abs/2302.05543},
 volume = {abs/2302.05543},
 year = {2023}
}

@software{adobephotoshop,
 author = {{Adobe Inc.}},
 date = {2019-03-06},
 title = {Adobe Photoshop},
 url = {https://www.adobe.com/products/photoshop.html},
 version = {CC 2019}
}

@article{alalufEthnicVariationMelanin2002a,
 abstract = {We have examined the quantity and composition of melanin in both photoprotected (volar upper arm) and chronically photoexposed (dorsal forearm) skin from a range of different ethnic skin types including African, Indian, Mexican, Chinese and European. The most lightly pigmented (European, Chinese and Mexican) skin types have approximately half as much epidermal melanin as the most darkly pigmented (African and Indian) skin types. However, the composition of melanin in these lighter skin types is comparatively more enriched with lightly coloured, alkali-soluble melanin components (up to three-fold). Regardless of ethnicity, epidermal melanin content is significantly greater in chronically photoexposed skin than it is in corresponding photoprotected skin (up to two-fold). However, by comparison there is only a modest enrichment of lightly coloured, alkali soluble melanin components in photoprotected skin (up to 1.3-fold). Analysis of melanosomes extracted from the epidermis in these subjects indicates that the proportion of spheroidal melanosomes is low in all skin types examined ({$<$}10\%). This suggests that in human skin, pheomelanin is a very minor component of epidermal melanin, even in the lightest (European) skin types. Analysis of melanosome size revealed a significant and progressive variation in size with ethnicity: African skin having the largest melanosomes followed in turn by Indian, Mexican, Chinese and European. On the basis of these findings, we propose that variation in skin pigmentation is strongly influenced by both the amount and the composition (or colour) of the melanin in the epidermis. Variation in melanosome size may also play a significant role. However, the data also suggest that in human skin there are subtle differences in the mechanisms associated with the maintenance of constitutive pigmentation and facultative hyperpigmentation, respectively.},
 author = {Alaluf, Simon and Atkins, Derek and Barrett, Karen and Blount, Margaret and Carter, Nik and Heath, Alan},
 date = {2002-04},
 doi = {10.1034/j.1600-0749.2002.1o071.x},
 file = {D\:\\Documents\\zotero\\storage\\34GTIQTD\\Alaluf 等 - 2002 - Ethnic Variation in Melanin Content and Compositio.pdf;D\:\\Documents\\zotero\\storage\\8RCFL46C\\alaluf2002.pdf.pdf},
 issn = {0893-5785, 1600-0749},
 journaltitle = {Pigment Cell Research},
 langid = {english},
 number = {2},
 pages = {112--118},
 shortjournal = {Pigment Cell Research},
 title = {Ethnic {{Variation}} in {{Melanin Content}} and {{Composition}} in {{Photoexposed}} and {{Photoprotected Human Skin}}},
 url = {https://onlinelibrary.wiley.com/doi/10.1034/j.1600-0749.2002.1o071.x},
 urldate = {2023-11-20},
 volume = {15}
}

@article{ANDERSON198113,
 author = {Anderson, R. Rox and Parrish, John A.},
 doi = {10.1111/1523-1747.ep12479191},
 issn = {0022-202X},
 journal = {Journal of Investigative Dermatology},
 number = {1},
 pages = {13--19},
 title = {The Optics of Human Skin},
 url = {https://www.sciencedirect.com/science/article/pii/S0022202X15461251},
 volume = {77},
 year = {1981}
}

@inproceedings{Bai_2023_CVPR,
 author = {Bai, Haoran and Kang, Di and Zhang, Haoxian and Pan, Jinshan and Bao, Linchao},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
 title = {FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction},
 year = {2023}
}

@article{beharaSkinLesionSynthesis2023,
 abstract = {The prognosis for patients with skin cancer improves with regular screening and checkups. Unfortunately, many people with skin cancer do not receive a diagnosis until the disease has advanced beyond the point of effective therapy. Early detection is critical, and automated diagnostic technologies like dermoscopy, an imaging device that detects skin lesions early in the disease, are a driving factor. The lack of annotated data and class-imbalance datasets makes using automated diagnostic methods challenging for skin lesion classification. In recent years, deep learning models have performed well in medical diagnosis. Unfortunately, such models require a substantial amount of annotated data for training. Applying a data augmentation method based on generative adversarial networks (GANs) to classify skin lesions is a plausible solution by generating synthetic images to address the problem. This article proposes a skin lesion synthesis and classification model based on an Improved Deep Convolutional Generative Adversarial Network (DCGAN). The proposed system generates realistic images using several convolutional neural networks, making training easier. Scaling, normalization, sharpening, color transformation, and median filters enhance image details during training. The proposed model uses generator and discriminator networks, global average pooling with 2 × 2 fractional-stride, backpropagation with a constant learning rate of 0.01 instead of 0.0002, and the most effective hyperparameters for optimization to efficiently generate high-quality synthetic skin lesion images. As for the classification, the final layer of the Discriminator is labeled as a classifier for predicting the target class. This study deals with a binary classification predicting two classes-benign and malignant-in the ISIC2017 dataset: accuracy, recall, precision, and F1-score model classification performance. BAS measures classifier accuracy on imbalanced datasets. The DCGAN Classifier model demonstrated superior performance with a notable accuracy of 99.38\% and 99\% for recall, precision, F1 score, and BAS, outperforming the state-of-the-art deep learning models. These results show that the DCGAN Classifier can generate high-quality skin lesion images and accurately classify them, making it a promising tool for deep learning-based medical image analysis.},
 author = {Behara, Kavita and Bhero, Ernest and Agee, John Terhile},
 date = {2023-08-09},
 doi = {10.3390/diagnostics13162635},
 eprint = {37627894},
 eprinttype = {pmid},
 file = {D:\Documents\zotero\storage\PU6TESRZ\Behara 等 - 2023 - Skin Lesion Synthesis and Classification Using an .pdf},
 issn = {2075-4418},
 journaltitle = {Diagnostics (Basel, Switzerland)},
 keywords = {convolutional neural network,deep learning,generative adversarial network,skin cancer},
 langid = {english},
 number = {16},
 pages = {2635},
 pmcid = {PMC10453872},
 shortjournal = {Diagnostics (Basel)},
 title = {Skin {{Lesion Synthesis}} and {{Classification Using}} an {{Improved DCGAN Classifier}}},
 volume = {13}
}

@inproceedings{bertalmio2001navier,
 author = {Bertalmio, Marcelo and Bertozzi, Andrea L and Sapiro, Guillermo},
 booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
 organization = {IEEE},
 pages = {I--I},
 title = {Navier-stokes, fluid dynamics, and image and video inpainting},
 volume = {1},
 year = {2001}
}

@incollection{chiang2016practical,
 author = {Chiang, Matt Jen-Yuan and Kutz, Peter and Burley, Brent},
 booktitle = {ACM SIGGRAPH 2016 Talks},
 pages = {1--2},
 title = {Practical and controllable subsurface scattering for production path tracing},
 year = {2016}
}

@inproceedings{d2007efficient,
 author = {d'Eon, Eugene and Luebke, David and Enderton, Eric},
 booktitle = {Proceedings of the 18th Eurographics conference on Rendering Techniques},
 pages = {147--157},
 title = {Efficient rendering of human skin},
 year = {2007}
}

@inproceedings{DBLP:conf/cvpr/SzegedyVISW16,
 author = {Christian Szegedy and
Vincent Vanhoucke and
Sergey Ioffe and
Jonathon Shlens and
Zbigniew Wojna},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/SzegedyVISW16.bib},
 booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
 doi = {10.1109/CVPR.2016.308},
 pages = {2818--2826},
 publisher = {{IEEE} Computer Society},
 timestamp = {Sun, 25 Oct 2020 01:00:00 +0200},
 title = {Rethinking the Inception Architecture for Computer Vision},
 url = {https://doi.org/10.1109/CVPR.2016.308},
 year = {2016}
}

@inproceedings{DBLP:conf/iccv/ZhuPIE17,
 author = {Jun{-}Yan Zhu and
Taesung Park and
Phillip Isola and
Alexei A. Efros},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iccv/ZhuPIE17.bib},
 booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
Italy, October 22-29, 2017},
 doi = {10.1109/ICCV.2017.244},
 pages = {2242--2251},
 publisher = {{IEEE} Computer Society},
 timestamp = {Thu, 11 Jan 2018 00:00:00 +0100},
 title = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial
Networks},
 url = {https://doi.org/10.1109/ICCV.2017.244},
 year = {2017}
}

@inproceedings{DBLP:conf/iclr/DumoulinSK17,
 author = {Vincent Dumoulin and
Jonathon Shlens and
Manjunath Kudlur},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/DumoulinSK17.bib},
 booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {A Learned Representation For Artistic Style},
 url = {https://openreview.net/forum?id=BJO-BuT1g},
 year = {2017}
}

@inproceedings{DBLP:conf/nips/HoJA20,
 author = {Jonathan Ho and
Ajay Jain and
Pieter Abbeel},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/HoJA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
 year = {2020}
}

@inproceedings{DBLP:journals/corr/abs-1812-04948,
 author = {Tero Karras and
Samuli Laine and
Timo Aila},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/KarrasLA19.bib},
 booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
2019, Long Beach, CA, USA, June 16-20, 2019},
 doi = {10.1109/CVPR.2019.00453},
 pages = {4401--4410},
 publisher = {Computer Vision Foundation / {IEEE}},
 timestamp = {Mon, 20 Jan 2020 00:00:00 +0100},
 title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
 url = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Karras\_A\_Style-Based\_Generator\_Architecture\_for\_Generative\_Adversarial\_Networks\_CVPR\_2019\_paper.html},
 year = {2019}
}

@inproceedings{DBLP:journals/corr/abs-1907-10786,
 author = {Yujun Shen and
Jinjin Gu and
Xiaoou Tang and
Bolei Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/ShenGTZ20.bib},
 booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2020, Seattle, WA, USA, June 13-19, 2020},
 doi = {10.1109/CVPR42600.2020.00926},
 pages = {9240--9249},
 publisher = {{IEEE}},
 timestamp = {Tue, 11 Aug 2020 01:00:00 +0200},
 title = {Interpreting the Latent Space of GANs for Semantic Face Editing},
 url = {https://doi.org/10.1109/CVPR42600.2020.00926},
 year = {2020}
}

@article{DBLP:journals/corr/GatysEB15a,
 author = {Leon A. Gatys and
Alexander S. Ecker and
Matthias Bethge},
 journal = {ArXiv preprint},
 title = {A Neural Algorithm of Artistic Style},
 url = {https://arxiv.org/abs/1508.06576},
 volume = {abs/1508.06576},
 year = {2015}
}

@inproceedings{DBLP:journals/corr/KingmaW13,
 author = {Diederik P. Kingma and
Max Welling},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
 booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
 title = {Auto-Encoding Variational Bayes},
 url = {http://arxiv.org/abs/1312.6114},
 year = {2014}
}

@article{doi:10.1080/10867651.2004.10487596,
 author = { Alexandru   Telea },
 doi = {10.1080/10867651.2004.10487596},
 journal = {Journal of Graphics Tools},
 number = {1},
 pages = {23-34},
 publisher = {Taylor & Francis},
 title = {An Image Inpainting Technique Based on the Fast Marching Method},
 url = {https://doi.org/10.1080/10867651.2004.10487596},
 volume = {9},
 year = {2004}
}

@article{doi:10.2352/EI.2023.35.7.IMAGE-276,
 abstract = {
Abstract
Simulating the effects of skincare products on the face is a potential new mode for product self-promotion while facilitating consumers to choose the right product. Furthermore, such simulations enable one to anticipate her skin condition and better manage skin health. However, there is a lack of effective simulations today. In this paper, we propose the first simulation model to reveal facial pore changes after using skincare products. Our simulation pipeline consists of two steps: training data establishment and facial pore simulation. To establish training data, we collect face images with various pore quality indexes from short-term (8-weeks) clinical studies. People experience significant skin fluctuations (due to natural rhythms, external stressors, etc.,) which introduce large perturbations, and we propose a sliding window mechanism to clean data and select representative index(es) to present facial pore changes. The facial pore simulation stage consists of 3 modules: UNet-based segmentation module to localize facial pores; regression module to predict time-dependent warping hyperparameters; and deformation module, taking warping hyperparameters and pore segmentation labels as inputs, to precisely deform pores accordingly. The proposed simulation renders realistic facial pore changes. This work will pave the way for future research in facial skin simulation and skincare product developments.
},
 author = {Ling Li and Bandara Dissanayake and Tatsuya Omotezako and Yunjie Zhong and Qing Zhang and Rizhao Cai and Qian Zheng and Dennis Sng and Weisi Lin and Yufei Wang and Alex C. Kot},
 doi = {10.2352/EI.2023.35.7.IMAGE-276},
 journal = {Electronic Imaging},
 keywords = {Short-term facial pore simulation, Facial pore segmentation, Skincare product efficacy, Facial pore warping, Skin health},
 number = {7},
 pages = {276-1--276-1},
 title = {Evaluating the efficacy of skincare product: A realistic short-term facial pore simulation},
 url = {https://library.imaging.org/ei/articles/35/7/IMAGE-276},
 volume = {35},
 year = {2023}
}

@inproceedings{doiSpectralEstimationHuman2003,
 abstract = {The present paper describes a method for modeling human skin coloring and estimating the surface-spectral reflectance by using the Kubelka-Munk theory. First, human skin is modeled as two layers of turbid materials. Second, we describe the reflectance estimation problem as the Kubelka-Munk equations with unknown six parameters. These parameters are the regular reflectance at skin surface and the five weights for spectral absorption of such different pigments as melanin, carotene, oxy-hemoglobin, deoxy-hemoglobin, and bilirubin. Moreover, the optical coefficients of spectral absorption and scattering for the two skin layers and the thickness values of these layers are used for the solution. Finally, experiments are done for estimating the skin surface-spectral reflectance on some body parts, such as the cheeks of human face, the palm, the backs of hand, the inside of arm, and the outside of arm. It is confirmed that the proposed method is more reliable in all cases.},
 author = {Doi, Motonori and Tominaga, Shoji},
 date = {2003-01-20},
 doi = {10.1117/12.472026},
 editor = {Eschbach, Reiner and Marcu, Gabriel G.},
 eventtitle = {Electronic {{Imaging}} 2003},
 langid = {english},
 location = {{Santa Clara, CA}},
 pages = {221},
 title = {Spectral Estimation of Human Skin Color Using the {{Kubelka-Munk}} Theory},
 url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.472026},
 urldate = {2023-04-16}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
 author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 date = {2014-06-10},
 doi = {10.48550/arXiv.1406.2661},
 eprint = {1406.2661},
 eprintclass = {cs, stat},
 eprinttype = {arxiv},
 title = {Generative {{Adversarial Networks}}},
 url = {http://arxiv.org/abs/1406.2661},
 urldate = {2022-10-31}
}

@inproceedings{heGuidedImageFiltering2010,
 abstract = {In this paper, we propose a novel type of explicit image filter - guided filter. Derived from a local linear model, the guided filter generates the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can perform as an edge-preserving smoothing operator like the popular bilateral filter [1], but has better behavior near the edges. It also has a theoretical connection with the matting Laplacian matrix [2], so is a more generic concept than a smoothing operator and can better utilize the structures in the guidance image. Moreover, the guided filter has a fast and non-approximate linear-time algorithm, whose computational complexity is independent of the filtering kernel size. We demonstrate that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications including noise reduction, detail smoothing/enhancement, HDR compression, image matting/feathering, haze removal, and joint upsampling.},
 author = {He, Kaiming and Sun, Jian and Tang, Xiaoou},
 booktitle = {Computer {{Vision}} – {{ECCV}} 2010},
 date = {2010},
 doi = {10.1007/978-3-642-15549-9_1},
 editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
 file = {D\:\\Documents\\zotero\\storage\\DLSXBUSZ\\He 等 - 2010 - Guided Image Filtering.pdf;D\:\\Documents\\zotero\\storage\\R4J7NQZT\\he2010.pdf.pdf},
 isbn = {978-3-642-15549-9},
 keywords = {Alpha Matte,Bilateral Filter,Dark Channel,Detail Layer,Local Linear Model},
 langid = {english},
 location = {{Berlin, Heidelberg}},
 pages = {1--14},
 publisher = {{Springer}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Guided {{Image Filtering}}}
}

@inproceedings{heusel2017gans,
 author = {Martin Heusel and
Hubert Ramsauer and
Thomas Unterthiner and
Bernhard Nessler and
Sepp Hochreiter},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/HeuselRUNH17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {6626--6637},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash
Equilibrium},
 url = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
 year = {2017}
}

@article{HYVARINEN2000411,
 abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
 author = {A. Hyvärinen and E. Oja},
 doi = {https://doi.org/10.1016/S0893-6080(00)00026-5},
 issn = {0893-6080},
 journal = {Neural Networks},
 keywords = {Independent component analysis, Projection pursuit, Blind signal separation, Source separation, Factor analysis, Representation},
 number = {4},
 pages = {411-430},
 title = {Independent component analysis: algorithms and applications},
 url = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
 volume = {13},
 year = {2000}
}

@inproceedings{Igarashi2005TheAO,
 author = {Takanori Igarashi and Ko Nishino and Shree K. Nayar},
 title = {The Appearance of Human Skin},
 url = {https://api.semanticscholar.org/CorpusID:16700137},
 year = {2005}
}

@inproceedings{isolaImagetoimageTranslationConditional2017,
 author = {Phillip Isola and
Jun{-}Yan Zhu and
Tinghui Zhou and
Alexei A. Efros},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/IsolaZZE17.bib},
 booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
{CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
 doi = {10.1109/CVPR.2017.632},
 pages = {5967--5976},
 publisher = {{IEEE} Computer Society},
 timestamp = {Thu, 16 Nov 2017 00:00:00 +0100},
 title = {Image-to-Image Translation with Conditional Adversarial Networks},
 url = {https://doi.org/10.1109/CVPR.2017.632},
 year = {2017}
}

@article{JIMENEZ2015_CGF,
 author = {Jorge Jimenez and Károly Zsolnai and Adrian Jarabo and Christian Freude and Thomas Auzinger and
Xian-Chun Wu and Javier von der Pahlen and Michael Wimmer and Diego Gutierrez},
 journal = {Computer Graphics Forum},
 title = {Separable Subsurface Scattering},
 year = {2015}
}

@article{jungDeepLearningbasedOptical2023a,
 abstract = {SIGNIFICANCE: Melanin and hemoglobin have been measured as important diagnostic indicators of facial skin conditions for aesthetic and diagnostic purposes. Commercial clinical equipment provides reliable analysis results, but it has several drawbacks: exclusive to the acquisition system, expensive, and computationally intensive. AIM: We propose an approach to alleviate those drawbacks using a deep learning model trained to solve the forward problem of light-tissue interactions. The model is structurally extensible for various light sources and cameras and maintains the input image resolution for medical applications. APPROACH: A facial image is divided into multiple patches and decomposed into melanin, hemoglobin, shading, and specular maps. The outputs are reconstructed into a facial image by solving the forward problem over skin areas. As learning progresses, the difference between the reconstructed image and input image is reduced, resulting in the melanin and hemoglobin maps becoming closer to their distribution of the input image. RESULTS: The proposed approach was evaluated on 30 subjects using the professional clinical system, VISIA VAESTRO. The correlation coefficients for melanin and hemoglobin were found to be 0.932 and 0.857, respectively. Additionally, this approach was applied to simulated images with varying amounts of melanin and hemoglobin. CONCLUSION: The proposed approach showed high correlation with the clinical system for analyzing melanin and hemoglobin distribution, indicating its potential for accurate diagnosis. Further calibration studies using clinical equipment can enhance its diagnostic ability. The structurally extensible model makes it a promising tool for various image acquisition conditions.},
 author = {Jung, Geunho and Kim, Semin and Lee, Jongha and Yoo, Sangwook},
 date = {2023-03},
 doi = {10.1117/1.JBO.28.3.035001},
 eprint = {36992693},
 eprinttype = {pmid},
 file = {D:\Documents\zotero\storage\US7FT6YH\Jung 等 - 2023 - Deep learning-based optical approach for skin anal.pdf},
 issn = {1560-2281},
 journaltitle = {Journal of Biomedical Optics},
 keywords = {deep learning,Deep Learning,Face,hemoglobin,Hemoglobins,Humans,light–tissue interaction,melanin,Melanins,Skin,skin analysis,VISIA},
 langid = {english},
 number = {3},
 pages = {035001},
 pmcid = {PMC10042298},
 shortjournal = {J Biomed Opt},
 title = {Deep Learning-Based Optical Approach for Skin Analysis of Melanin and Hemoglobin Distribution},
 volume = {28}
}

@article{linExemplarbasedFreckleRetouching2019,
 author = {Lin, Tsung-Ying and Tsai, Yu-Ting and Huang, Tsung-Shian and Lin, Wen-Chieh and Chuang, Jung-Hong},
 date = {2019-02},
 doi = {10.1016/j.cag.2018.11.002},
 file = {D\:\\Documents\\zotero\\storage\\EHGBZAWP\\187e6865937bb134c33b545eec38a99a.pdf.pdf;D\:\\Documents\\zotero\\storage\\UP3ALH67\\Lin 等 - 2019 - Exemplar-based freckle retouching and skin tone ad.pdf},
 issn = {00978493},
 journaltitle = {Computers \& Graphics},
 langid = {english},
 pages = {54--63},
 shortjournal = {Computers \& Graphics},
 title = {Exemplar-Based Freckle Retouching and Skin Tone Adjustment},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S0097849318301808},
 urldate = {2023-12-07},
 volume = {78}
}

@inproceedings{lipowezkyAutomaticFrecklesDetection2008,
 abstract = {Freckles and blemishes are one of important features of human appearance, which could be used for a variety of potential applications such as face recognition, malignant melanoma early diagnosis and concealing cream simulation. This paper introduces novel approach to skin freckles extraction, based on precise open skin detection using integration of texture, shape and color features. The proposed approach allows detecting and retouching each freckle separately and independently one from another. The process is starting with face location and exact typical facial features extraction such as open skin, eyes and mouth and finishing with freckle masks. Freckles retouching procedure is twofold: firstly the skin inside of freckle mask is replaced with surrounding not freckle skin and then alpha-blending with the original photo is applied in order to simulate the power of concealing cream. Experiments with real human photos show high effectiveness of the proposed technique.},
 author = {Lipowezky, Uri and Cahen, Sarah},
 booktitle = {2008 {{IEEE}} 25th {{Convention}} of {{Electrical}} and {{Electronics Engineers}} in {{Israel}}},
 date = {2008-12},
 doi = {10.1109/EEEI.2008.4736675},
 eventtitle = {2008 {{IEEE}} 25th {{Convention}} of {{Electrical}} and {{Electronics Engineers}} in {{Israel}}},
 file = {D\:\\Documents\\zotero\\storage\\B6DB6SQI\\lipowezky2008.pdf.pdf;D\:\\Documents\\zotero\\storage\\CHSHUYXK\\4736675.html},
 pages = {142--146},
 title = {Automatic Freckles Detection and Retouching},
 url = {https://ieeexplore.ieee.org/document/4736675},
 urldate = {2023-12-10}
}

@article{Liu2017AutomaticFF,
 author = {Xin Liu and Lu Xie and Bineng Zhong and Jixiang Du and Qinmu Peng},
 journal = {IET Image Process.},
 pages = {1068-1076},
 title = {Automatic facial flaw detection and retouching via discriminative structure tensor},
 url = {https://api.semanticscholar.org/CorpusID:45218701},
 volume = {11},
 year = {2017}
}

@inproceedings{liuAutomaticBeautificationGroupPhoto2018,
 abstract = {Directly benefiting from the powerful generative adversarial networks (GANs) in recent years, various new image processing tasks pertinent to image generation and synthesis have gained more popularity with the growing success. One such application is individual portrait photo beautification based on facial expression detection and editing. Yet, automatically beautifying group photos without tedious and fragile human interventions still remains challenging. The difficulties inevitably arise from diverse facial expression evaluation, harmonious expression generation, and context-sensitive synthesis from single/multiple photos. To ameliorate, we devise a two-stage deep network for automatic group-photo evaluation and beautification by seamless integration of multi-label CNN with Bayesian network enhanced GANs. First, our multi-label CNN is designed to evaluate the quality of facial expressions. Second, our novel Bayesian GANs framework is proposed to automatically generate photo-realistic beautiful expressions. Third, to further enhance naturalness of beautified group photos, we embed Poisson fusion in the final layer of the GANs in order to synthesize all the beautified individual expressions. We conducted extensive experiments on various kinds of single-/multi-frame group photos to validate our novel network design. All the experiments confirm that, our novel method can uniformly accommodate diverse expression evaluation and generation/synthesis of group photos, and outperform the state-of-the-art methods in terms of effectiveness, versatility, and robustness.},
 author = {Liu, Ji and Li, Shuai and Song, Wenfeng and Liu, Liang and Qin, Hong and Hao, Aimin},
 booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2018},
 date = {2018},
 doi = {10.1007/978-3-030-01418-6_74},
 file = {D:\Documents\zotero\storage\KJGTBF3L\liu2018.pdf.pdf},
 isbn = {978-3-030-01418-6},
 keywords = {Bayesian networks,Beautification of group-photo facial expressions,Generative adversarial networks,Multi-label CNN,Poisson fusion},
 langid = {english},
 location = {{Cham}},
 pages = {760--770},
 publisher = {{Springer International Publishing}},
 series = {Lecture {{Notes}} in {{Computer Science}}},
 title = {Automatic {{Beautification}} for {{Group-Photo Facial Expressions Using Novel Bayesian GANs}}}
}

@inproceedings{luFacialSkinBeautification2016,
 abstract = {In this paper, we propose a facial skin beautification framework to remove facial spots based on layer dictionary learning and sparse representation. More precisely, we first decompose the face image into three layers: lighting layer, detail layer and color layer. The corresponding detail layer dictionary are learned by using 60 thousands beauty images collected from the Internet. Thereafter, the detail layer of the image is reconstructed by using sparse representation. Moreover, a binary mask obtained from the learned layer is used to transform detail information from original detail layer to the learned one. The experiment results demonstrate that the proposed method is more effective in eliminating moles, flaws and wrinkles in face image compared with representative commercial systems like PicTreat, Portrait+, Portraitrue and MeituPic.},
 author = {Lu, Xi and Chang, Xiaobin and Xie, Xiaohua and Hu, Jian-Fang and Zheng, Wei-Shi},
 booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
 date = {2016-07},
 doi = {10.1109/IJCNN.2016.7727515},
 eventtitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
 file = {D:\Documents\zotero\storage\K493I5KS\lu2016.pdf.pdf},
 issn = {2161-4407},
 pages = {2534--2539},
 title = {Facial Skin Beautification via Sparse Representation over Learned Layer Dictionary},
 url = {https://ieeexplore.ieee.org/document/7727515},
 urldate = {2023-12-10}
}

@software{newville_matthew_2014_11813,
 author = {Newville, Matthew and
Stensitzki, Till and
Allen, Daniel B. and
Ingargiola,  Antonino},
 doi = {10.5281/zenodo.11813},
 publisher = {Zenodo},
 title = {{LMFIT: Non-Linear Least-Square Minimization and 
Curve-Fitting for Python}},
 url = {https://doi.org/10.5281/zenodo.11813},
 version = {0.8.0},
 year = {2014}
}

@misc{rombach2021highresolution,
 archiveprefix = {arXiv},
 author = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
 eprint = {2112.10752},
 primaryclass = {cs.CV},
 title = {High-Resolution Image Synthesis with Latent Diffusion Models},
 year = {2021}
}

@inproceedings{ronneberger2015unet,
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 booktitle = {International Conference on Medical image computing and computer-assisted intervention},
 organization = {Springer},
 pages = {234--241},
 title = {U-net: Convolutional networks for biomedical image segmentation},
 year = {2015}
}

@article{scikit-learn,
 author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal = {Journal of Machine Learning Research},
 pages = {2825--2830},
 title = {Scikit-learn: Machine Learning in {P}ython},
 volume = {12},
 year = {2011}
}

@inproceedings{shafaeiAutoRetouchAutomaticProfessional2021,
 author = {Shafaei, Alireza and Little, James J. and Schmidt, Mark},
 booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
 date = {2021-01},
 doi = {10.1109/WACV48630.2021.00103},
 eventtitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
 file = {D\:\\Documents\\zotero\\storage\\6VKSPBRP\\Shafaei 等 - 2021 - AutoRetouch Automatic Professional Face Retouchin.pdf;D\:\\Documents\\zotero\\storage\\D7I94DXL\\shafaei2021.pdf.pdf},
 isbn = {978-1-66540-477-8},
 langid = {english},
 location = {{Waikoloa, HI, USA}},
 pages = {989--997},
 publisher = {{IEEE}},
 shorttitle = {{{AutoRetouch}}},
 title = {{{AutoRetouch}}: {{Automatic Professional Face Retouching}}},
 url = {https://ieeexplore.ieee.org/document/9423125/},
 urldate = {2023-12-10}
}

@article{tsumura1999independent,
 author = {Tsumura, Norimichi and Haneishi, Hideaki and Miyake, Yoichi},
 journal = {JOSA A},
 number = {9},
 pages = {2169--2176},
 publisher = {Optica Publishing Group},
 title = {Independent-component analysis of skin color image},
 volume = {16},
 year = {1999}
}

@article{tsumuraImagebasedSkinColor,
 abstract = {This paper proposes an E-cosmetic function for digital images based on physics and physiologically-based image processing. A practical skin color and texture analysis/synthesis technique is introduced for this E-cosmetic function. Shading on the face is removed by a simple color vector analysis in the optical density domain as an inverse lighting technique. The image without shading is analyzed by a previously introduced technique that extracts hemoglobin and melanin components by independent component analysis. Experimental results using UV-B irradiation and the application of methyl nicotinate on the arms support the physiological validity of the analysis and the effectiveness of the proposed shading removal. We synthesized the way facial images changed due to tanning or alcohol consumption, and compared the synthesized images with images of actual changes in skin color. The comparison shows an excellent match between the synthesized and actual images of changes due to tanning and alcohol consumption. We also proposed a technique to synthesize the change of texture in pigment due to aging or the application of cosmetics. The pyramid-based texture analysis/synthesis technique was used for the spatial processing of texture. Using the proposed technique, we could realistically change the skin color and texture of a 50 year-old woman to that of a 20 year-old woman.},
 author = {Tsumura, Norimichi and Ojima, Nobutoshi and Sato, Kayoko and Shiraishi, Mitsuhiro and Shimizu, Hideto and Nabeshima, Hirohide and Akazaki, Syuuichi and Hori, Kimihiko and Miyake, Yoichi},
 file = {D:\Documents\zotero\storage\8KLG2L2I\Tsumura 等。 - Image-based skin color and texture analysissynthe.pdf},
 langid = {english},
 pages = {10},
 title = {Image-Based Skin Color and Texture Analysis/Synthesis by Extracting Hemoglobin and Melanin Information in the Skin}
}

@inproceedings{velusamyFabSoftenFaceBeautification2020,
 abstract = {Face retouching is a widespread application in modern smartphone cameras with its high business value evidenced by its broad user base. We propose a real-time face softening approach that smooths blemishes in the facial skin region, followed by a wavelet band manipulation to restore the underlying skin texture, which produces a highly appealing ‘beautified’ face that retains its natural appearance. Softening is carried out by an attribute-aware dynamic smoothing filter that is guided by facial attributes, including the number of blemishes and coarseness of facial skin texture. The proposed solution is robust to wide variations in lighting conditions, skin nonuniformities, blemishes, the presence of facial accessories, and delicate hairlike regions. The method includes an explicit facial hair preservation module to preserve their delicate texture while smoothing blemishes. We perform a qualitative comparison of our proposed face softening approach with numerous state-of-the-art techniques and commercial products. We demonstrate the power of our method in producing beautified faces at a minimal performance cost, which enables smooth execution on low-power devices like smartphones.},
 author = {Velusamy, Sudha and Parihar, Rishubh and Kini, Raviprasad and Rege, Aniket},
 booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
 date = {2020-06},
 doi = {10.1109/CVPRW50498.2020.00273},
 eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
 file = {D\:\\Documents\\zotero\\storage\\JS3DBVPD\\10.1109@CVPRW50498.2020.00273.pdf.pdf;D\:\\Documents\\zotero\\storage\\UGJ9BQFY\\Velusamy 等 - 2020 - FabSoften Face Beautification via Dynamic Skin Sm.pdf},
 isbn = {978-1-72819-360-1},
 langid = {english},
 location = {{Seattle, WA, USA}},
 pages = {2248--2256},
 publisher = {{IEEE}},
 shorttitle = {{{FabSoften}}},
 title = {{{FabSoften}}: {{Face Beautification}} via {{Dynamic Skin Smoothing}}, {{Guided Feathering}}, and {{Texture Restoration}}},
 url = {https://ieeexplore.ieee.org/document/9150825/},
 urldate = {2023-12-08}
}

@article{Whitmore2000,
 author = {Whitmore,  S.Elizabeth and Sago,  N.Joan Glavan},
 doi = {10.1016/s0190-9622(00)90012-4},
 issn = {0190-9622},
 journal = {Journal of the American Academy of Dermatology},
 number = {1},
 pages = {76–79},
 publisher = {Elsevier BV},
 title = {Caliper-measured skin thickness is similar in white and black women},
 url = {http://dx.doi.org/10.1016/s0190-9622(00)90012-4},
 volume = {42},
 year = {2000}
}

@incollection{wrenninge2017path,
 author = {Wrenninge, Magnus and Villemin, Ryusuke and Hery, Christophe},
 booktitle = {Tech. Rep.},
 publisher = {Pixar Inc.},
 title = {Path traced subsurface scattering using anisotropic phase functions and non-exponential free flights},
 year = {2017}
}

@inproceedings{xieBlemishawareProgressiveFace2023,
 abstract = {Face retouching aims to remove facial blemishes, while at the same time maintaining the textual details of a given input image. The main challenge lies in distinguishing blemishes from the facial characteristics, such as moles. Training an image-to-image translation network with pixelwise supervision suffers from the problem of expensive paired training data, since professional retouching needs specialized experience and is time-consuming. In this paper, we propose a Blemish-aware and Progressive Face Retouching model, which is referred to as BPFRe. Our framework can be partitioned into two manageable stages to perform progressive blemish removal. Specifically, an encoder-decoder-based module learns to coarsely remove the blemishes at the first stage, and the resulting intermediate features are injected into a generator to enrich local detail at the second stage. We find that explicitly suppressing the blemishes can contribute to an effective collaboration among the components. Toward this end, we incorporate an attention module, which learns to infer a blemish-aware map and further determine the corresponding weights, which are then used to refine the intermediate features transferred from the encoder to the decoder, and from the decoder to the generator. Therefore, BPFRe is able to deliver significant performance gains on a wide range of face retouching tasks. It is worth noting that we reduce the dependence of BPFRe on paired training samples by imposing effective regularization on unpaired ones.},
 author = {Xie, Lianxin and Xue, Wen and Xu, Zhen and Wu, Si and Yu, Zhiwen and Wong, Hau San},
 booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
 date = {2023-06},
 doi = {10.1109/CVPR52729.2023.00542},
 eventtitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
 file = {D:\Documents\zotero\storage\5KX6XLXS\Xie 等 - 2023 - Blemish-aware and Progressive Face Retouching with.pdf},
 isbn = {9798350301298},
 langid = {english},
 location = {{Vancouver, BC, Canada}},
 pages = {5599--5608},
 publisher = {{IEEE}},
 title = {Blemish-Aware and {{Progressive Face Retouching}} with {{Limited Paired Data}}},
 url = {https://ieeexplore.ieee.org/document/10203154/},
 urldate = {2023-12-07}
}
@inproceedings{krishnapriyaAnalysisManualAutomated2022a,
  title = {Analysis of {{Manual}} and {{Automated Skin Tone Assignments}}},
  booktitle = {2022 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision Workshops}} ({{WACVW}})},
  author = {Krishnapriya, K. S. and Pangelinan, Gabriella and King, Michael C. and Bowyer, Kevin W.},
  date = {2022-01},
  pages = {429--438},
  publisher = {{IEEE}},
  location = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACVW54805.2022.00049},
  url = {https://ieeexplore.ieee.org/document/9707510/},
  urldate = {2023-12-22},
  abstract = {The Fitzpatrick scale is a standard tool in dermatology to classify skin types for melanin and sensitivity to sun exposure. After an in-person interview, the dermatologist would classify the person’s skin type on a six-valued, light-to-dark scale. Various face image analysis researchers have recently categorized skin tone in face images on a six-valued, light-to-dark scale in order to look into questions of bias and accuracy related to skin tone. Categorization of skin tone on the basis of images rather than personal interview is not, on that basis alone, strictly speaking, on the Fitzpatrick scale. While the manual assignment of face images on a six-point, light-to-dark scale has been used by various researchers studying bias in face image analysis, to date there has been no study on the consistency and reliability of observers assigning skin type from an image. We analyze a set of manual skin type assignments from multiple observers viewing the same image set and find that there are inconsistencies between human raters. We then develop an algorithm for automated skin type assignments, which could be used in place of manual assignment by observers. Such an algorithm would allow for provision of skin tone annotations on large quantities of images beyond what could be accomplished by manual raters. To our knowledge, this is the first work to: (a) examine the consistency of manual skin tone ratings across observers, (b) document that there is substantial variation in the rating of the same image by different observers even when exemplar images are given for guidance and all images are color-corrected, and (c) compare manual versus automated skin tone ratings. We release the automated skin tone rating implementation so that other researchers may reproduce and extend the results in this paper.},
  eventtitle = {2022 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision Workshops}} ({{WACVW}})},
  isbn = {978-1-66545-824-5},
  langid = {english},
  file = {D:\Documents\zotero\storage\NY9BTQLW\Krishnapriya 等 - 2022 - Analysis of Manual and Automated Skin Tone Assignm.pdf}
}

@article{spigulis2017smartphone,
  title={Smartphone snapshot mapping of skin chromophores under triple-wavelength laser illumination},
  author={Spigulis, Janis and Oshina, Ilze and Berzina, Anna and Bykov, Alexander},
  journal={Journal of Biomedical Optics},
  volume={22},
  number={9},
  pages={091508--091508},
  year={2017},
  publisher={Society of Photo-Optical Instrumentation Engineers}
}